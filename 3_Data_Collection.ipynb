{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection\n",
    "===\n",
    "Up until now, the data that you've been working with has been given to in the form of a text file, a database, or in a python pickle. In this module, we'll learn a few techniques for writing our own python scripts for collecting data from the internet. \n",
    "\n",
    "Collecting data from the internet can be done in one of two ways:\n",
    "1. through an API (Application Program Interface)\n",
    "2. by parsing html\n",
    "\n",
    "The API route is always preferable as it is (usually) sactioned by whomever is hosting the data, which allows them to dictate terms of use and monitor and restrict how their service is used. An access token (or \"key\") is typically required to access data from an official API.\n",
    "\n",
    "Not all websites provide a public API for accessing data. In those cases, you may have to write a custom HTML parser. HTML is the language that the static components of webpages are written in. This code is usually well structured and it is often times not difficult to write a short script to find the elements of a interest in the HTML.\n",
    "\n",
    "Getting Data via an API\n",
    "---\n",
    "Each API is different as is the process getting an API key. Usually getting an API key is as simple as creating an account and registering for a key. Almost all companies that offer a public API, also offer documentation on how to use their API and how to get a key. Here's Goodreads' Developer's page: https://www.goodreads.com/api\n",
    "\n",
    "We will want to retrieve a Goodreads API key. In order to do so, create an account with Goodreads, and then head to the developer page and apply for an API key. \n",
    "\n",
    ">**Note:** Most API users have their own web application and are using the Goodreads API to allow the users of their web app to integrate their goodreads data with their app. The client ID and client secret are specific to the web application and the access token is specific to the user of the web application. You can think of the access token as a custom username and password that the client can use on behalf of its users to get access to certain data that is only available to a user logged into goodreads. Something similar happens everytime you use facebook or google to \"login\" to a third party site. The third party side gets an access token from facebook or google and can use that to get your name, birthdate, gender or any other information that you've agreed to give it access to.\n",
    "Although we won't be making any requests on anybody elses behalf, we'll still need an access token to access goodreads data as though we were logged in.\n",
    "\n",
    "When filling out the form to register your client you will only need to fill in an application name and company name. Feel free to leave the other fields blank. Agree to the terms of service listed below and apply. Goodreads will instantly provide you with an API key and a client secret. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get data from Goodreads, we'll be making \"requests\" to their API. We'll be making our requests through python, although you could also make a request directly from your browser. Your browser is in fact making dozens or even hundreds of requests every time it loads a web page. In the cases where your browser loads a web page, it's converting HTML into well formatted human-readable content. When we make requests to an API we'll usually get back content that was designed for a computer to read. Paste the following url (replacing YOUR-API-KEY with your API key) into your browser.\n",
    "\n",
    "````\n",
    "https://www.goodreads.com/book/review_counts.json?isbns=0141439602&key=YOUR-API-KEY\n",
    "\n",
    "````\n",
    "\n",
    "What you get back is a JSON object that contains review counts for the book `A Tale of Two Cities`. An ISBN is essentially a serial number for a book. In order to know which book the ISBN refers to, we used the 'book.show' method listed in the Goodreads API documentation. Goodreads lists all available methods here. Replacing the book 'id' in the sample URL returns metadata around the book in question, including the title. \n",
    "\n",
    "We briefly saw JSON objects in the previous module. They're identical in structure to python dictionaries, and python comes with a library for converting JSON strings into a dictionary.\n",
    "\n",
    "Let's make that same API request, now using python's `requests` library. Create a variable using your API key, so that we can easily substitute in the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'{\"books\":[{\"id\":1953,\"isbn\":\"0141439602\",\"isbn13\":\"9780141439600\",\"ratings_count\":552448,\"reviews_count\":873363,\"text_reviews_count\":8614,\"work_ratings_count\":581897,\"work_reviews_count\":957951,\"work_text_reviews_count\":10860,\"average_rating\":\"3.78\"}]}'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "my_key = \"miQ3kVWsjPTA7UmAWM8Dg\"\n",
    "url = \"https://www.goodreads.com/book/review_counts.json?isbns=0141439602&key=\"+my_key\n",
    "response = requests.get(url)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response object has other metadata about the response (success/error codes, headers, etc.), but we're only interested in the content (or \"text\") of the response. As we saw before, the response text is a JSON string. We'll use python's `json` library to turn that string into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'books': [{u'average_rating': u'3.78',\n",
       "   u'id': 1953,\n",
       "   u'isbn': u'0141439602',\n",
       "   u'isbn13': u'9780141439600',\n",
       "   u'ratings_count': 552448,\n",
       "   u'reviews_count': 873363,\n",
       "   u'text_reviews_count': 8614,\n",
       "   u'work_ratings_count': 581897,\n",
       "   u'work_reviews_count': 957951,\n",
       "   u'work_text_reviews_count': 10860}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "response_data = json.loads(response.text)\n",
    "response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': '200 OK', 'x-request-id': '1WBFB1VP06Y9XM6BM6FN', 'x-xss-protection': '1; mode=block', 'x-content-type-options': 'nosniff, nosniff', 'content-encoding': 'gzip', 'set-cookie': 'csid=BAhJIhg5MTEtNjI1ODM1My0wMTE2NzM4BjoGRVQ%3D--b9491e8a0a5a6f555f2081872818441467ede936; path=/; expires=Sat, 15 Dec 2035 15:50:01 -0000, locale=en; path=/, _session_id2=e04b0663638384f87a36f4fa9f0b260c; path=/; expires=Tue, 15 Dec 2015 21:50:01 -0000; HttpOnly', 'vary': 'Accept-Encoding,User-Agent', 'content-length': '160', 'server': 'Server', 'x-runtime': '0.028118', 'etag': 'W/\"65747fc8dfcac017cf4c2657fda8daf1-gzip\"', 'cache-control': 'max-age=0, private, must-revalidate', 'date': 'Tue, 15 Dec 2015 15:50:01 GMT', 'x-frame-options': 'ALLOWALL', 'content-type': 'application/json; charset=utf-8'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which we saw how to work with in the previous module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'3.78'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ave_rating = response_data['books'][0]['average_rating']\n",
    "ave_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** The \"books\" value is actually a list of dictionaries (with only one element), we need to specify that we want the first element in that list. \n",
    "\n",
    "This is the Basic workflow for using python to get data from the internet:\n",
    "1. Make a request and get the content\n",
    "2. convert the content into a parsable python object (like a dictionary)\n",
    "3. get the data of interest and do something with it\n",
    "\n",
    "**Exercise 3.1:**\n",
    "Write a function that takes a list of goodreads ISBNs and returns the goodreads Ids.\n",
    "\n",
    "\n",
    "###API Parameters\n",
    "Instagram's [recent media user endpoint](https://instagram.com/developer/endpoints/users/#get_users_media_recent) returns recent posts for a given user id. By default, it will return 20 of the most recent posts, but you can supply parameters to override some of the defaults. For instance, if you wanted the last 20 posts from 2014, you include the `max_timestamp` parameter in your request with value 1420070400.\n",
    "\n",
    ">**side note: what is a unix timestamp?**  A unix timestamp is the number of seconds since January 1, 1970. While it's a pretty inconvenient way to represent time from the point-of-view of human readability, you'll often see it used in data storage because it's easier to use in calculations, takes up less space, and is faster to lookup than a datetime string. Python's `datetime` library has a few functions for converting to and from unix time. You can also do these conversion for unix timestamps stored in a dataframe with pandas. And, if you have a single unix timestamp that you're going to lookup just once, you can also do so manually [here](http://www.epochconverter.com).\n",
    "\n",
    "The API parameters are passed in as a suffix to the url. The first parameter is preceded with a `?` and all subsequent parameters are separated by an `&`. All api request require at least one parameter (the access token). A request url using the `max_timestamp` parameter would look like this:\n",
    "````\n",
    "https://api.instagram.com/v1/users/260133476/media/recent/?access_token=703990239.f1d54a3.8cf94c42373e43a2951bce01d7ac0bf1&max_timestamp=1420070400\n",
    "````\n",
    "You can paste this url in your browser to see the results (be sure the use your own access_token). Note that despite what is implied by the instagram api documentation, your request params should be all lowercase.\n",
    "\n",
    ">**PRO-TIP:** There are browser extensions that make it much easier to view json in your browser (everything is well-formatted and you can collapse values). For those of you who use the chrome web browser, [here's the json formatting extentsion that I use](https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc?hl=en). You may have to restart your to start the extension.\n",
    "\n",
    "**Exercise 3.2:** Get the most recent 30 instagram posts by The Alabama Shakes (instagram username: alabama_shakes). Record the id, the link, the number of comments, the number of likes, and the filter. Put the results in a dataframe.\n",
    "\n",
    "###Pagination\n",
    "An API request doesn't always return all availalbe data. For instance, by default you only get the 20 most recent posts from Instagram's recent media endpoint, but even using the `count` param you can only get a maximum of 33 posts per request. One solution to get data for all posts is to use the `max_id` param and pass it the lowest id of your current results and continue to do this (using a `while-loop`) until the API returns an empty data set. If you're lucky, the API results will include a paginiation link that you can follow to get the next page for you. Then to get all of the results you'd just continue to follow the pagination link until there were no more pagination links.\n",
    "\n",
    "**Exercise 3.3:** Get *every* instagram post by the Alabama Shakes and put the same values from above in another dataframe. Determine which was the bands most popular post and what their favorite filter is.\n",
    "\n",
    "###Rate Limits\n",
    "One important consideration when building a crawler that will make lots of API requests is that it adheres to the rate limits (i.e. number of requests per hour) [specified in the documentation](https://www.goodreads.com/api/terms). You can do this either pro-actively or reactively. The proactive solution is to put a pause between each request. Goodreads has a 1 request per second rate limit. You can use the `sleep` function from python's `time` library to implement these pauses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "response = requests.get(url)  # make a request\n",
    "time.sleep(1)  # sleep for 1 second\n",
    "response = requests.get(url)  # make the next request request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes an API response will give you feedback too let you know that you've hit their rate-limit. In that case you can take a reactive approach to limiting your requests. Instagram provides this feedback in the form of a response code (which is part of python's response object). When all is well, the code is 200, but if you've exceeded your rate limit, the code will change to 429."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code  # code should be 200 if all is well and 429 if we've exceeded our rate limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** exceeding the rate limit isn't the only thing that can go wrong with an API call and there's a [different status code for each issue you might have](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes). For instance, if we make a request to the recent media endpoint using an invalid user id, we get a 400 status code (bad request). If we make a request to a non-exising endpoint, we get a 404 (Not Found) status code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.goodreads.com/book/review_counts.json?isbns=-1&key=\"+my_key  # -1 is not a valid ISBN\n",
    "response = requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing HTML for data\n",
    "---\n",
    "An official API is always the best way to get data from a host, but sometimes a website either doesn't have an API or, if they do have an API, some data of interest may not be available via an API endpoint. In those cases you may be able to get the raw HTML and use an HTML parsing library to extract the data of interest.\n",
    "\n",
    "We can get the raw HTML, the same we we got API data, with the `requests` library. Let's grab the HTML from the wikipedia page on Conan episodes from 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_Conan_episodes_(2014)\"\n",
    "response = requests.get(url)\n",
    "html_raw = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response text is raw html which is difficult to work with, so we'll use the [beautiful soup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) library to help us parse out the data we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # To get everything\n",
    "html_soup = BeautifulSoup(html_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML is a nested set of opening and closing tags which define the page layout and properties. Everything is typically wrapped in html open (<code>&lt;html></code>) and close (<code>&lt;/html></code>) tags. Between <code>&lt;html>&lt;/html></code> there's usually a <code>&lt;head>&lt;/head></code> section where metadata and javascript code lives, and a <code>&lt;body>&lt;/body></code> section where the page content lives. Once you've soupified your html, You can access nested elements as though they were attributes of their parent object. For instance, to get the `head` section of the page you would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head = html_soup.html.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and to get the title element within the `head` section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>List of Conan episodes (2014) - Wikipedia, the free encyclopedia</title>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also perform a search on a beautiful soup object. Let's say you want to get the Table of Contents element from this wikipedia page. Suppose you know that the Table of Contents HTML element has an `id` attribute with value \"toc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table_of_contents = html_soup.find(id='toc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful soup objects have a few useful methods and attributes. The `text` attribute is a string of all text within the element that would be displayed in a browser, and the `attrs` attibute is a dictionary of html attributes that are a part of the tag (like the `id` or `class`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': ['toc'], 'id': 'toc'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_of_contents.attrs  # the table of contents element has 2 attributes, a class (equal to ['toc']) and an 'id' (equal to 'toc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "\n",
      "1 2014\n",
      "\n",
      "1.1 January\n",
      "1.2 February\n",
      "1.3 March\n",
      "1.4 April\n",
      "1.5 May\n",
      "1.6 June\n",
      "1.7 July\n",
      "1.8 August\n",
      "1.9 September\n",
      "1.10 October\n",
      "1.11 November\n",
      "1.12 December\n",
      "\n",
      "\n",
      "2 References\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print table_of_contents.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to find *all* instances of an element meeting our search criteria we can instead use the `findAll` method. Let's get a list of all months included in the Table of Contents. If you print the `table_of_contents` object, you'll see that each month is wrapped in a list tag (<code>&lt;li></code>) with class attribute 'toclevel-2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_elements = table_of_contents.findAll('li', {'class':'toclevel-2'})\n",
    "len(month_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there are 12 month_elements. Let's see what the first month_element looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li class=\"toclevel-2 tocsection-2\"><a href=\"#January\"><span class=\"tocnumber\">1.1</span> <span class=\"toctext\">January</span></a></li>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_elements[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each month element has a tag (a link to somewhere else in the page in this case) and two spans. The text of the spans are the table of content number and month. Let's create a lists for the href, number, and month and put the results in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>href</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.1</th>\n",
       "      <td>#January</td>\n",
       "      <td>January</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2</th>\n",
       "      <td>#February</td>\n",
       "      <td>February</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.3</th>\n",
       "      <td>#March</td>\n",
       "      <td>March</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.4</th>\n",
       "      <td>#April</td>\n",
       "      <td>April</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.5</th>\n",
       "      <td>#May</td>\n",
       "      <td>May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.6</th>\n",
       "      <td>#June</td>\n",
       "      <td>June</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.7</th>\n",
       "      <td>#July</td>\n",
       "      <td>July</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.8</th>\n",
       "      <td>#August</td>\n",
       "      <td>August</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.9</th>\n",
       "      <td>#September</td>\n",
       "      <td>September</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.10</th>\n",
       "      <td>#October</td>\n",
       "      <td>October</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.11</th>\n",
       "      <td>#November</td>\n",
       "      <td>November</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.12</th>\n",
       "      <td>#December</td>\n",
       "      <td>December</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            href      month\n",
       "1.1     #January    January\n",
       "1.2    #February   February\n",
       "1.3       #March      March\n",
       "1.4       #April      April\n",
       "1.5         #May        May\n",
       "1.6        #June       June\n",
       "1.7        #July       July\n",
       "1.8      #August     August\n",
       "1.9   #September  September\n",
       "1.10    #October    October\n",
       "1.11   #November   November\n",
       "1.12   #December   December"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrefs = []\n",
    "toc_numbers = []\n",
    "months  = []\n",
    "for element in month_elements:\n",
    "    a_tag = element.a  # a tags in html are used for links within a page or too another page\n",
    "    hrefs.append(a_tag.attrs['href'])  # the href attribute determines where the link goes\n",
    "    toc_number = element.find('span',{'class':'tocnumber'})\n",
    "    toc_numbers.append(toc_number.text)\n",
    "    month = element.find('span',{'class':'toctext'})\n",
    "    months.append(month.text)\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame({'month':months, 'href':hrefs}, index=toc_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that with the `find` and `findAll` methods, we can specified both the tag type (a, span, div, etc.) as well as attribute values. It's worth looking over the [beautiful soup Quick Start](http://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start) documentation to see all of the available methods. If you're not very familiar with HTML you might also consider looking over [this tutorial](http://www.w3schools.com/html/) or taking the [codeacademy course](https://www.codecademy.com/learn/web) in HTML/CSS for a deeper dive.\n",
    "\n",
    ">**Pro-Tip:** Most web browsers have their own built-in developer tools for investegating all the HTML and other elements that go into displaying a web page. In Chrome you can right click on any element on a page and click on \"Inspect Element\" to look at it's HTML. Use the arrows on the left of each element to expand or collapse in order to show/hide all of its children.\n",
    "\n",
    "**Exercise 3.4:** Use beautiful soup to get the show number (\"No.\"), air date, lists of guests, and list of entertainment guests for every Conan show from 2014. Put the results in a dataframe.  **Bonus:** make the date a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
