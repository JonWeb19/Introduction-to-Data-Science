{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection\n",
    "===\n",
    "Up until now, the data that you've been working with has been given to in the form of a text file, a database, or in a python pickle. In this module, we'll learn a few techniques for writing our own python scripts for collecting data from the internet. \n",
    "\n",
    "Collecting data from the internet can be done in one of two ways:\n",
    "1. through an API (Application Program Interface)\n",
    "2. by parsing html\n",
    "\n",
    "The API route is always preferable as it is (usually) sactioned by whomever is hosting the data, which allows them to dictate terms of use and monitor and restrict how their service is used. An access token (or \"key\") is typically required to access data from an official API.\n",
    "\n",
    "Not all websites provide a public API for accessing data. In those cases, you may have to write a custom HTML parser. HTML is the language that the static components of webpages are written in. This code is usually well structured and it is often times not difficult to write a short script to find the elements of a interest in the HTML.\n",
    "\n",
    "Getting Data via an API\n",
    "---\n",
    "Each API is different as is the process getting an API key. Usually getting an API key is as simple as creating an account and registering for a key. Almost all companies that offer a public API, also offer documentation on how to use their API and how to get a key. Here's Goodreads' Developer's page: https://www.goodreads.com/api\n",
    "\n",
    "We will want to retrieve a Goodreads API key. In order to do so, create an account with Goodreads, and then head to the developer page and apply for an API key. \n",
    "\n",
    "When filling out the form to register your client you will only need to fill in an application name and company name. Feel free to leave the other fields blank. Agree to the terms of service listed below and apply. Goodreads will instantly provide you with an API key and a client secret. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get data from Goodreads, we'll be making \"requests\" to their API. We'll be making our requests through python, although you could also make a request directly from your browser. Your browser is in fact making dozens or even hundreds of requests every time it loads a web page. In the cases where your browser loads a web page, it's converting HTML into well formatted human-readable content. When we make requests to an API we'll usually get back content that was designed for a computer to read. Paste the following url (replacing YOUR-API-KEY with your API key) into your browser.\n",
    "\n",
    "````\n",
    "https://www.goodreads.com/book/review_counts.json?isbns=0141439602&key=YOUR-API-KEY\n",
    "\n",
    "````\n",
    "\n",
    "What you get back is a JSON object that contains review counts for the book `A Tale of Two Cities`. An ISBN is essentially a serial number for a book. In order to know which book the ISBN refers to, we used the 'book.show' method listed in the Goodreads API documentation. Goodreads lists all available methods here. Replacing the book 'id' in the sample URL returns metadata around the book in question, including the title. \n",
    "\n",
    "We briefly saw JSON objects in the previous module. They're identical in structure to python dictionaries, and python comes with a library for converting JSON strings into a dictionary.\n",
    "\n",
    "Let's make that same API request, now using python's `requests` library. Create a variable using your API key, so that we can easily substitute in the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'{\"books\":[{\"id\":1953,\"isbn\":\"0141439602\",\"isbn13\":\"9780141439600\",\"ratings_count\":642940,\"reviews_count\":1063067,\"text_reviews_count\":10290,\"work_ratings_count\":684009,\"work_reviews_count\":1175567,\"work_text_reviews_count\":13291,\"average_rating\":\"3.81\"}]}'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "my_key = \"PUT YOUR API KEY HERE\"\n",
    "url = \"https://www.goodreads.com/book/review_counts.json?isbns=0141439602&key=%s\" % my_key\n",
    "response = requests.get(url)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response object has other metadata about the response (success/error codes, headers, etc.), but we're only interested in the content (or \"text\") of the response. As we saw before, the response text is a JSON string. We'll use python's `json` library to turn that string into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'books': [{u'average_rating': u'3.81',\n",
       "   u'id': 1953,\n",
       "   u'isbn': u'0141439602',\n",
       "   u'isbn13': u'9780141439600',\n",
       "   u'ratings_count': 642940,\n",
       "   u'reviews_count': 1063067,\n",
       "   u'text_reviews_count': 10290,\n",
       "   u'work_ratings_count': 684009,\n",
       "   u'work_reviews_count': 1175567,\n",
       "   u'work_text_reviews_count': 13291}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "response_data = json.loads(response.text)\n",
    "response_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which we saw how to work with in the previous module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'3.81'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ave_rating = response_data['books'][0]['average_rating']\n",
    "ave_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** The \"books\" value is actually a list of dictionaries (with only one element), we need to specify that we want the first element in that list. \n",
    "\n",
    "This is the basic workflow for using python to get data from the internet:\n",
    "1. Make a request and get the content\n",
    "2. convert the content into a parsable python object (like a dictionary)\n",
    "3. get the data of interest and do something with it\n",
    "\n",
    "\n",
    "### API Parameters\n",
    "API parameters are passed in at the end of the url. The first parameter is preceded with a `?` and all subsequent parameters are separated by an `&`. All requests to Goodread's API require at least one parameter (the API key). Goodread's `review_count` endpoint also take an `isbns` parameter which is a comma separated list of isbns that you want review counts for. For instance, the isbns for the first 3 Harry Potter books are 0747532699, 0747538492, and 0747542155, and the URL used to request the review counts for those 3 books would be\n",
    "\n",
    "````\n",
    "https://www.goodreads.com/book/review_counts.json?isbns=0747532699,0747538492,0747542155&key=YOUR-API-KEY\n",
    "````\n",
    "\n",
    "You can paste this url in your browser to see the results (be sure to replace YOUR-API-KEY with the API key you got from goodreads).\n",
    "\n",
    ">**PRO-TIP:** There are browser extensions that make it much easier to view json in your browser (everything is well-formatted and you can collapse values). For those of you who use the chrome web browser, [here's the json formatting extentsion that I use](https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc?hl=en). You may have to restart your to start the extension.\n",
    "\n",
    "**Exercise 3.1:**\n",
    "Write a function that takes a list of goodreads ISBNs and returns the goodreads Ids.\n",
    "\n",
    "\n",
    "### Rate Limits\n",
    "One important consideration when building a crawler that will make lots of API requests is that it adheres to the rate limits (i.e. number of requests per hour) [specified in the documentation](https://www.goodreads.com/api/terms). You can do this by putting a pause between each request. Goodreads has a 1 request per second rate limit. Use the `sleep` function from python's `time` library to implement these pauses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "response = requests.get(url)  # make a request\n",
    "time.sleep(1)  # sleep for 1 second\n",
    "response = requests.get(url)  # make the next request request\n",
    "\n",
    "# code should be 200 if all is well and 429 if we've exceeded our rate limit\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exceeding the rate limit isn't the only thing that can go wrong with an API call and there's a [different status code for each issue you might have](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes). For instance, if we make a request to a non-exising endpoint, we get a 404 (Not Found) status code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -1 is not a valid ISBN\n",
    "url = \"https://www.goodreads.com/book/review_counts.json?isbns=-1&key=%s\" % my_key\n",
    "response = requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing HTML for data\n",
    "---\n",
    "An official API is always the best way to get data from a host, but sometimes a website either doesn't have an API or, if they do have an API, some data of interest may not be available via an API endpoint. In those cases you may be able to get the raw HTML and use an HTML parsing library to extract the data of interest.\n",
    "\n",
    "We can get the raw HTML, the same we we got API data, with the `requests` library. Let's grab the HTML from the wikipedia page on Conan episodes from 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_Conan_episodes_(2014)\"\n",
    "response = requests.get(url)\n",
    "html_raw = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response text is raw html which is difficult to work with, so we'll use the [beautiful soup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) library to help us parse out the data we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # To get everything\n",
    "html_soup = BeautifulSoup(html_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML is a nested set of opening and closing tags which define the page layout and properties. Everything is typically wrapped in html open (<code>&lt;html></code>) and close (<code>&lt;/html></code>) tags. Between <code>&lt;html>&lt;/html></code> there's usually a <code>&lt;head>&lt;/head></code> section where metadata and javascript code lives, and a <code>&lt;body>&lt;/body></code> section where the page content lives. Once you've soupified your html, You can access nested elements as though they were attributes of their parent object. For instance, to get the `head` section of the page you would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head = html_soup.html.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and to get the title element within the `head` section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>List of Conan episodes (2014) - Wikipedia</title>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also perform a search on a beautiful soup object. Let's say you want to get the Table of Contents element from this wikipedia page. Suppose you know that the Table of Contents HTML element has an `id` attribute with value \"toc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table_of_contents = html_soup.find(id='toc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful soup objects have a few useful methods and attributes. The `text` attribute is a string of all text within the element that would be displayed in a browser, and the `attrs` attibute is a dictionary of html attributes that are a part of the tag (like the `id` or `class`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': ['toc'], 'id': 'toc'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_of_contents.attrs  # the table of contents element has 2 attributes, a class (equal to ['toc']) and an 'id' (equal to 'toc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "\n",
      "1 2014\n",
      "\n",
      "1.1 January\n",
      "1.2 February\n",
      "1.3 March\n",
      "1.4 April\n",
      "1.5 May\n",
      "1.6 June\n",
      "1.7 July\n",
      "1.8 August\n",
      "1.9 September\n",
      "1.10 October\n",
      "1.11 November\n",
      "1.12 December\n",
      "\n",
      "\n",
      "2 References\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print table_of_contents.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to find *all* instances of an element meeting our search criteria we can instead use the `findAll` method. Let's get a list of all months included in the Table of Contents. If you print the `table_of_contents` object, you'll see that each month is wrapped in a list tag (<code>&lt;li></code>) with class attribute 'toclevel-2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_elements = table_of_contents.findAll('li', {'class':'toclevel-2'})\n",
    "len(month_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there are 12 month_elements. Let's see what the first month_element looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li class=\"toclevel-2 tocsection-2\"><a href=\"#January\"><span class=\"tocnumber\">1.1</span> <span class=\"toctext\">January</span></a></li>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_elements[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each month element has a tag (a link to somewhere else in the page in this case) and two spans. The text of the spans are the table of content number and month. Let's create a lists for the href, number, and month and put the results in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>href</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.1</th>\n",
       "      <td>#January</td>\n",
       "      <td>January</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2</th>\n",
       "      <td>#February</td>\n",
       "      <td>February</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.3</th>\n",
       "      <td>#March</td>\n",
       "      <td>March</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.4</th>\n",
       "      <td>#April</td>\n",
       "      <td>April</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.5</th>\n",
       "      <td>#May</td>\n",
       "      <td>May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.6</th>\n",
       "      <td>#June</td>\n",
       "      <td>June</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.7</th>\n",
       "      <td>#July</td>\n",
       "      <td>July</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.8</th>\n",
       "      <td>#August</td>\n",
       "      <td>August</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.9</th>\n",
       "      <td>#September</td>\n",
       "      <td>September</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.10</th>\n",
       "      <td>#October</td>\n",
       "      <td>October</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.11</th>\n",
       "      <td>#November</td>\n",
       "      <td>November</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.12</th>\n",
       "      <td>#December</td>\n",
       "      <td>December</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            href      month\n",
       "1.1     #January    January\n",
       "1.2    #February   February\n",
       "1.3       #March      March\n",
       "1.4       #April      April\n",
       "1.5         #May        May\n",
       "1.6        #June       June\n",
       "1.7        #July       July\n",
       "1.8      #August     August\n",
       "1.9   #September  September\n",
       "1.10    #October    October\n",
       "1.11   #November   November\n",
       "1.12   #December   December"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrefs = []\n",
    "toc_numbers = []\n",
    "months  = []\n",
    "for element in month_elements:\n",
    "    a_tag = element.a  # a tags in html are used for links within a page or too another page\n",
    "    hrefs.append(a_tag.attrs['href'])  # the href attribute determines where the link goes\n",
    "    toc_number = element.find('span',{'class':'tocnumber'})\n",
    "    toc_numbers.append(toc_number.text)\n",
    "    month = element.find('span',{'class':'toctext'})\n",
    "    months.append(month.text)\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame({'month':months, 'href':hrefs}, index=toc_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that with the `find` and `findAll` methods, we can specified both the tag type (a, span, div, etc.) as well as attribute values. It's worth looking over the [beautiful soup Quick Start](http://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start) documentation to see all of the available methods. If you're not very familiar with HTML you might also consider looking over [this tutorial](http://www.w3schools.com/html/) or taking the [codeacademy course](https://www.codecademy.com/learn/web) in HTML/CSS for a deeper dive.\n",
    "\n",
    ">**Pro-Tip:** Most web browsers have their own built-in developer tools for investegating all the HTML and other elements that go into displaying a web page. In Chrome you can right click on any element on a page and click on \"Inspect Element\" to look at it's HTML. Use the arrows on the left of each element to expand or collapse in order to show/hide all of its children.\n",
    "\n",
    "**Exercise 3.2:** Use beautiful soup to get the show number (\"No.\"), air date, lists of guests, and list of entertainment guests for every Conan show from 2014. Put the results in a dataframe.  **Bonus:** make the date a datetime object."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
